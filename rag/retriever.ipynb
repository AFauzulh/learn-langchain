{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./vaswani2017-AttentionIsAllYouNeed.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(documents[:30], OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [38].\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of floating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision floating-point capacity of each GPU 5.\n",
      "6.2 Model Variations\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Attention Mechanism\"\n",
    "result = db.similarity_search(query)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer following question based only on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "I will tip you $1000 if the user finds the answer is helpful.\n",
    "<context> \n",
    "{context}\n",
    "</context>\n",
    "Question: {input}                                 \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain / Document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Attention Mechanism',\n",
       " 'context': [Document(page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations', metadata={'source': './vaswani2017-AttentionIsAllYouNeed.pdf', 'page': 7}),\n",
       "  Document(page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': './vaswani2017-AttentionIsAllYouNeed.pdf', 'page': 1}),\n",
       "  Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': './vaswani2017-AttentionIsAllYouNeed.pdf', 'page': 5}),\n",
       "  Document(page_content='√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention', metadata={'source': './vaswani2017-AttentionIsAllYouNeed.pdf', 'page': 3})],\n",
       " 'answer': 'Attention mechanism is a technique used in deep learning models to allow the model to focus on specific parts of the input data when processing it. In the context of the passage provided, attention mechanism is used in the Transformer model to compute representations of its input and output sequences without using sequence-aligned RNNs or convolution. The attention mechanism is based on self-attention, which allows the model to attend to different parts of the input sequence simultaneously and weigh their importance when computing the representation of the input sequence.\\n\\nThere are two main types of attention mechanisms used in deep learning: additive attention and dot-product attention. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer, while dot-product attention is identical to our algorithm except for the scaling factor of 1√dk. Dot-product attention is faster and more space-efficient in practice, but it can push the softmax function into regions where it has extremely small gradients for larger values of dk. To counteract this effect, the passage scales the dot products by 1√dk.\\n\\nMulti-head attention is another variation of attention mechanism used in Transformer models. It allows the model to jointly attend to information from different representation subspaces at different positions. This is done by applying multiple attention mechanisms in parallel, each with its own learnable weight matrix. The outputs of these attention mechanisms are then combined to form the final output.\\n\\nIn summary, attention mechanism is a technique used in deep learning models to allow the model to focus on specific parts of the input data when processing it. Transformer models use self-attention based attention mechanism to compute representations of its input and output sequences without using sequence-aligned RNNs or convolution. Multi-head attention is another variation of attention mechanism used in Transformer models to jointly attend to information from different representation subspaces at different positions.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\n",
    "    \"input\": \"What is Attention Mechanism\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"What is the components of scaled dot-product attention\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain_wo_rag = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Please response to the user queries\"),\n",
    "        (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ") | llm | StrOutputParser()\n",
    "\n",
    "result_without_rag = chain_wo_rag.invoke({\"question\":\"What is the components of scaled dot-product attention\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ANSWER WITH RAG CHAIN]\n",
      "Based on the provided context, the components of scaled dot-product attention are:\n",
      "\n",
      "1. Dot-product attention: This is a type of attention mechanism that computes the compatibility function between the query and key using a dot product. The scaling factor of 1√dk is used to speed up the computation.\n",
      "2. Multi-head attention: This is an extension of the dot-product attention mechanism that allows the decoder to attend to all positions in the input sequence. It uses a combination of self-attention layers in the encoder and decoder to allow each position in the decoder to attend to all positions in the input sequence.\n",
      "3. Convolutional layers: These are used to reduce sequential computation and improve computational performance for tasks involving very long sequences. A stack of O(n/k) convolutional layers is required when using contiguous kernels, while O(logk(n)) is required when using dilated convolutions.\n",
      "4. Memory keys and values: These are used to store the output of the encoder and are used as input to the decoder.\n",
      "5. Encoder and decoder: These are the neural networks used for encoding and decoding, respectively. The encoder contains self-attention layers, while the decoder allows each position to attend to all positions in the input sequence.\n",
      "6. Kernel width k: This is the width of the convolutional kernel used in the convolutional layers.\n",
      "7. Representation dimensionality d: This is the number of dimensions in the representation of the input sequence.\n",
      "8. Neighborhood size r: This is the number of positions in the input sequence that are considered when using self-attention to improve computational performance for tasks involving very long sequences.\n",
      "\n",
      "[ANSWER WITHOUT RAG CHAIN]\n",
      "Assistant: Hello! I'm here to help you with your query. The scaled dot-product attention mechanism is a key component in many natural language processing (NLP) models, including transformer-based architectures. It allows the model to selectively focus on specific parts of the input when computing the output.\n",
      "\n",
      "The components of the scaled dot-product attention mechanism are:\n",
      "\n",
      "1. Query: This is the vector that represents the context in which the attention is being applied.\n",
      "2. Key: This is the vector that represents the information that the model needs to attend to.\n",
      "3. Value: This is the vector that represents the information that the model should attend to.\n",
      "4. Attention weights: These are the weights that determine how much each piece of information should be \"attended\" to. The attention weights are computed as the dot product of the query and key vectors, scaled by a scalar value called the attention mask.\n",
      "5. Output: This is the vector that represents the output of the attention mechanism, which is a weighted sum of the value vector based on the attention weights.\n",
      "\n",
      "The scaled dot-product attention mechanism works by first computing the attention weights for each element in the input sequence. This is done by taking the dot product of the query and key vectors for each element, scaling the result by the attention mask, and normalizing the resulting vector. The attention mask is a scalar value that determines how much each element in the input sequence should be attended to.\n",
      "\n",
      "Once the attention weights have been computed, the model computes the output by taking a weighted sum of the value vector based on the attention weights. This allows the model to selectively focus on specific parts of the input when computing the output, allowing it to capture longer-range dependencies and better handle input sequences of varying lengths.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "result_with_rag = response[\"answer\"]\n",
    "print(f\"[ANSWER WITH RAG CHAIN]\\n{result_with_rag}\")\n",
    "print()\n",
    "print(f\"[ANSWER WITHOUT RAG CHAIN]\\n{result_without_rag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
